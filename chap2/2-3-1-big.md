# 2.3.1 Big
> 大型数据集是达到目的的一种手段，他们本身并不是目的。

大数据源最受热议的特性是它“大”。很多论文中会强调，有时是炫耀，他们分析了多大的数据。例如，一篇用Google Books语料库研究文字使用倾向的文章，它发表在 *Science* 上（[Michel et al. 2011](https://doi.org/10.1126/science.1199644)）：
> 我们的语料库有超过5000亿字，其中英语（3610亿），法语（450亿），德语（370亿），中文（130亿），俄语（350亿），以及希伯来语（20亿）。其中最古老的书发表于1500年。最早的几十年只里每年只有几本书，大约有几十万字。到1800年，语料库每年增长9.8亿字；到1900年，18亿字，2000年，110亿字。地球人是无法读这语料库的。如果你尝试仅仅读2000年发表的英语文章，以一个合理的速度，200字/每分钟，废寝忘食的读，大约需要80年。这些文字序列的长度比人类的基因组1000倍还长：如果你把它们写成一行，大约有往返月球10次那么长。

这些数据的规模无疑令人印象深刻，很幸运，Google Books团队公开了这些数据（本章的一些扩展活动还会用到这些数据）。但是，当你看到这种炫耀时，你应该想想：是否所有的这些数据都有用？如果只有往返月球一次的数据量，我们能做同样的研究吗？如果这些数据只能到达珠穆朗玛峰或者埃菲尔铁塔，我们还能做同样的研究吗？

还有，这些研究的一些发现，要求语料库有很长的时间跨度。例如，他们尝试探索一些语法的演变，特别是不规则动词的变化规则的改变情况。由于一些不规则动词十分少见，因此，为发现它们使用情况随时间的改变，需要很多的数据。然而研究者们常常将大数据作为终极目标——“看！我用了这么多的数据！”——而不是关注更重要的科学目标。

在我的经验中，研究罕见事件是大数据带来的三个机遇中的一个。第二个是研究差异性，可以通过Raj Chetty 与其同事（[2014](https://doi.org/10.1093/qje/qju022)）的一项对美国社会流动性的研究来说明。过去，很多研究者通过比较两代人的收入来研究社会流动性。根据这篇文章（[Hout and Diprete, 2006](https://doi.org/10.1016/j.rssm.2005.10.001)），有优势的父母，倾向于有一个有优势的孩子，但这种关系的强度随着时间和地区的不同而不同。但是最近，Chetty与其同事可以用400万人的税收记录来估计美国各地区的代际流动性的差异性（图2.1）。例如，它们发现来自于国民收入比重倒数20%的家庭的孩子，后来进入国民收入比重前20%的，在San Jose, California大约有13%，但是在Charlotte, North Carolina只有4%。如果你看一下图2.1，你会开始想，为什么有些地区的代际流动性比其他地区高。Chetty也有同样的疑问，它们发现，高流动性的区域居住区隔离，收入不平等情况更少，同时有更好的小学，更好的基础设施以及更稳定的家庭。当然，这些相关性并不能说明这些因素会导致更高的流动性，但它确实指出了可能的机制，为之后的研究提供了方向，这正是Chetty后续工作所做的。注意，这个项目中数据量的大小是至关重要的。如果Chetty只有4万人而不是4000万人的税收记录，它们就不能估计出区域差异性，当然就更不可能在之后的工作中研究差异性背后的机制。

![图2.1](https://www.bitbybitbook.com/figures/chapter2/bitbybit2-1_heterogeneity_chetty.png)
> 图2.1：对来自收入倒数20%家庭而进入收入前20%的孩子的估计结果（[Chetty et al. 2014](https://doi.org/10.1093/qje/qju022)）。通过区域层次的预测，显示出差异性。自然而然的引出更有趣更重要的问题，这是国家层次无法看到的。之所以能做区域层次的预测，部分原因是研究人员使用了很大的数据源：4000万人的税收记录。所用数据公开在：http://www.equality-of-opportunity.org/

除了研究罕见事件和差异性，大型数据集也是研究人员能发现微小的差异。事实上，很多工业界对大数据的关注点就在这些细微差异上：稳定的检测广告点击率0.1%的差异，能带来额外的数百万美元的收益。然而在某些科研方面，这种细微差距就不那么重要，即使它是统计上显著的（[Prentice and Miller 1992](https://doi.org/10.1037/0033-2909.112.1.160)）。但是在某些政策制定上，这种细微差异就至关重要。例如，如果某个公共健康干预方案比另一个稍稍高效一丢丢，那么选择高效的干预方案会多拯救上千生命。

尽管庞大的数据量通常来说是优点，但我发现有时这会导致一些概念上的错误。由于某些神秘力量，庞大的数据量会使研究者忽视这些数据是如何产生的。尽管庞大的数据量消灭了随机误差，但需要留意是否引起了系统性的误差。下面这个例子就是忽略了数据的产生方式。[Back, Küfner, and Egloff 2010](https://doi.org/10.1177/0956797610382124)试图通过使用2001年9月11日的短信数据，生成一个恐怖袭击的高分辨率的情感时间线。由于研究者使用了大量的短信数据，他们不需要担心能否用随机变化来解释当天怒气值增长的模式。这数据量很大，模式也很明显，并且所有的统计性检验都指出这是真实的模式。但统计性检验忽略了这数据是如何产生的。事实上，最后发现很多模式都可以归咎于当天产生越来越多无意义短信的机器人。移除这个机器人完全摧毁了这篇论文的一些关键发现（[Pury 2011](https://doi.org/10.1177/0956797611408735); [Back, Küfner, and Egloff 2011](https://doi.org/10.1177/0956797611409592)）。简单的说，没有意识到到大数据带来的系统性误差的风险，很可能使他们对无关紧要的指标进行精确估计，例如一个机器人产生的无意义信息的情感倾向。

总的来说，大型数据集不是终极目标，但它为开启了一些特定的研究方向。例如研究罕见事件，估计差异性，以及检测细微的变化。注意大数据的隐患，它使得一些研究者忽略了数据产生的方式，这有可能导致研究者关注于某些无关紧要的指标。
